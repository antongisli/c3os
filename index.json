[
{
	"uri": "https://docs.c3os.io/architecture/container/",
	"title": "Container based OS",
	"tags": [],
	"description": "",
	"content": "c3os is a container-based OS.\nA container based OS is an OS which is shipped via containers. Indeed, if it happens to be based on Linux (most probably) you can run the container image as well on your docker daemon. The image being being booted is the container, which contains all the required pieces in order to boot (Kernel, Initrd, Init system).\nBenefits of using containers to distribute upgrades Container registries are already widely supported and used by anyone.\nIf you are operating a Kubernetes cluster and deploying apps on top, chances are that you already have a container registry deployed somewhere and configured to store them or manage your infrastructure stack. By using container images lets you re-use the same infrastructure to propagate upgrades to the nodes and handle customizations.\nContainers images can be extended after build by using standard container building practices, and seamlessly plug into your existing pipelines. c3OS allows to seamlessly upgrade to container images that are derived from other versions. This means that customizing a c3OS version requires just to build a standard container image with a plain Dockerfile, plus the bits that are actually needed.\nIf you are familiar with Dockerfiles, then you are good to go to roll your own custom OS version to provision in the nodes. That removes any friction to questions like \u0026ldquo;How do I add this package to my nodes?\u0026rdquo;, or more complex ones as \u0026ldquo;How can I replace with my own Kernel?\u0026rdquo;.\n"
},
{
	"uri": "https://docs.c3os.io/architecture/immutable/",
	"title": "Immutable layout",
	"tags": [],
	"description": "",
	"content": "c3os adopts an Immutable layout, and derivatives created with its toolkit inherits the same immutability aspects.\nAn immutable OS is a carefully engineered system which boots in a restricted, permissionless mode, where certain paths of the system are not writeable. For instance, after installation it\u0026rsquo;s not possible to install additional packages in the system, and any configuration change is discarded after reboot.\nA running Linux based OS system will look like with the following paths:\n/usr/local - persistent ( partition label COS_PERSISTENT) /oem - persistent ( partition label COS_OEM) /etc - ephemeral /usr - read only / immutable /usr/local will contain all the persistent data which will be carried over in-between upgrades, instead, any change to /etc will be discarded.\nBenefits of using an Immutable system There are many reasons why you would like so, and this is a genuine, good question. There are various perspective you can see at this, from a security standpoint, it is far more secure than traditional systems - most of attack vectors relies on writing on the system, or either installing persistent tools after a vector has been exploited.\nFrom a maintenance perspective, configuration management tools like Chef, Puppet, or alikes are not needed as Immutable systems have only a configuration entrypoint, every other configuration is cleaned up automatically after a reboot.\nThe benefit of rolling out the same system over a set of machines are obvious:\n No snowflakes - all the machines ships the same image, configuration settings, and behavior. this allows to have a predictable infrastructure, predictable upgrades and homogeneus configurations across your cluster Configuration is driven via cloud-init. There is only one source of truth for configuration, and that does happen at bootstrap time. Anything else it\u0026rsquo;s handled afterwards natively via Kubernetes, so no configuration management software is required. Reduced attack surface - Immutable systems cannot be modified or tampered on runtime. This enhances the security of a running OS as changes on the system are not allowed.  Tools like Chef, Puppet and Ansible share the same underlying issues when it comes to configuration management: nodes can have different versions matrix of software and OS, which makes your set of nodes dishomogeneous and difficult to maintain and orchestrate from day 1 to day 2.\nc3os tackles the issue from another angle, as can turn any distribution to an “immutable” system, distributed as a standard container image, which gets provisioned to the devices as declared. This allows to treat OSes with the same repeatable portability as containers for apps, removing snowflakes in your cluster. Container registries can be used either internally or externally to the cluster to propagate upgrades with customized versions of the OS (kernel, packages, etc).\n"
},
{
	"uri": "https://docs.c3os.io/quickstart/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Introduction The c3OS relases ships for user convenience a set of artifacts that can be used to install c3os on a node. However, c3OS Kubernetes Native components allows to create such artifacts inside Kubernetes from a set of input images.\nIn the quickstart below we will use the artifacts generated by the c3os releases as we assume we don\u0026rsquo;t have any prior Kubernetes cluster available.\nMost of the operations are completely automated, so there is little interaction needed with the Kubernetes components.\nInstallation can be also completely driven by the Kubernetes Native components if you have already a Kubernetes cluster to manage installations, with zero touch configuration. See the CRD quickstart (WIP).\nThe goal of this quickstart is to install c3os on a node and create a single-node Kubernetes cluster with k3s.\nTo simplify, we will install c3os inside a VM, however, your mileage and configuration may vary based on your setup, see the documentation for a more exhaustive list of examples.\nPrerequisites  A VM hypervisor that boots ISOs A machine where to run the c3os-cli (optional, we will see) A configuration file (cloud-init)  Download c3OS can be used to turn any distro in an immutable system, however, for user convenience there are several artifacts published as part of the releases to get started.\nYou can find the latest releases in the release page over Github. For instance, if we would like to pick the alpine based version, we would download the c3os-alpine-v0.57.0-k3sv1.21.14+k3s1.iso ISO file, where v1.21.14+k3s1 in the name is the k3s version and v0.57.0 is the c3os one.\nThe releases in the c3os-io/c3os repository are the c3os core images that ship without k3s and p2p full-mesh functionalities, however further extensions can be installed dynamically in runtime by using the c3os bundles mechanism.\nThe releases in c3os-io/provider-c3os instead ships k3s and p2p full-mesh support that needs to be explictly enabled. In follow-up releases there will be available also k3s-only artifacts.\n Booting up Download the ISO, and boot it up in to the hypervisor of your choice. If it\u0026rsquo;s a baremetal, you can flash the image directly into an usb stick with dd:\ndd if=/path/to/iso of=/path/to/dev bs=4MB or just use Etcher.\nYou should be greeted with a GRUB boot menu: There are several entries, depending on how you plan to install c3os. Let\u0026rsquo;s pick the first entry, or wait the default timeout. The machine will boot, and eventually a QR code will be printed out of the screen:\nConfiguration At this stage the machine is waiting for the configuration to continue futher with the installation process. Configuration can be either served via QR code, or manually via logging into the box and starting the installation process with a config file. The config file is a YAML file mixed with cloud-init syntax and the config of c3os itself.\nIn this example we will configure the node as a single-node kubernetes cluster, so we enable k3s, and we set a default password for the c3os user to later access to the box, alongside with some ssh keys:\n#node-config stages: initramfs: - name: \u0026#34;User settings\u0026#34; hostname: \u0026#34;hostname.domain.tld\u0026#34; authorized_keys: c3os: - \u0026#34;ssh-rsa AAA...\u0026#34; - \u0026#34;github:mudler\u0026#34; users: c3os: passwd: \u0026#34;c3os\u0026#34; dns: path: /etc/resolv.conf nameservers: - 8.8.8.8 k3s: enabled: true Save the config file as config.yaml as we will use it later in the process.\nNote:\n The stages.initramfs block will configure the c3os user (default) with the c3os password. Note, the c3os user is already configured with sudo. authorized_keys can be used to add additional keys to the user in order to SSH into hostname sets the machine hostname dns sets the DNS for the machine k3s.enabled=true enables k3s.  Several configuration can be added at this stage. See the configuration reference for further reading.\n Deployment Below there are instruction using the c3os-cli. To install by logging over SSH into the box see Manual install or Interactive install for driving the installation manually from the console.\n To trigger the installation process via QR code you need to use the c3os-cli, the cli is currently available only for Linux and Windows. It can be downloaded from the releases artifact:\ncurl -L https://github.com/c3os-io/provider-c3os/releases/download/v0.57.0/c3os-cli-v0.57.0-Linux-x86_64.tar.gz -o - | tar -xvzf - -C . The CLI allows to register a node with a screenshot, an image, or a token. During pairing the configuration is sent over and the node will continue the installation process.\nIn a terminal in your desktop/workstation, run:\nc3os register --reboot --device /dev/sda --config config.yaml Note:\n By default the CLI will take automatically a screenshot to get the QR code. Make sure to get that fit into the screen. Alternatively an image path or a token can be supplied via args (e.g. c3os register /img/path or c3os register \u0026lt;token\u0026gt;) The --reboot flag will make the node reboot automatically after the installation is completed The --device flag will instruct to install c3os in the specified drive, replace /dev/sda with your drive. It will take over and overwrite any content, please be cautious. The --config flag is used to specify the config file to drive the installation  Wait now for few minutes for the config to propagate to the node, the installation should start then and automatically reboot afterward.\nAccessing the node After boot the node will start and load into the system. When the console is available we should be already able to SSH.\nTo access to the host, login as c3os:\nssh c3os@IP Note:\n sudo is configured for the c3os user  You should be greeted with a welcome message:\nWelcome to c3os! Refer to https://docs.c3os.io for documentation. c3os@c3os:~\u0026gt; At this point, it can take few moments to get the k3s server running, but eventually we should be able to inspect the service and see k3s running, for example with systemd-based flavors:\n$ sudo systemctl status k3s ● k3s.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/k3s.service.d └─override.conf Active: active (running) since Thu 2022-09-01 12:02:39 CEST; 4 days ago Docs: https://k3s.io Main PID: 1834 (k3s-server) Tasks: 220 The k3s kubeconfig file is available at /etc/rancher/k3s/k3s.yaml. Please refer to the k3s documentation.\nSee also There are other ways to install c3os:\n Automate installation via cloud-config Manual installation Enable P2P mesh Take over installation Raspberry PI Netboot (TODO) CAPI Lifecycle management (TODO)  What\u0026rsquo;s next?  Upgrade nodes with Kubernets Upgrade nodes manually Immutable architeture  "
},
{
	"uri": "https://docs.c3os.io/upgrade/manual/",
	"title": "Manual",
	"tags": [],
	"description": "",
	"content": "Upgrades can be run manually from the terminal.\nc3os images are released on quay.io.\nList available versions To see all the available versions:\n$ sudo c3os-agent upgrade list-releases v0.57.0 v0.57.0-rc2 v0.57.0-rc1 v0.57.0-alpha2 v0.57.0-alpha1 Upgrade To upgrade to latest available version, run from a shell of a cluster node:\nsudo c3os-agent upgrade To specify a version, just run\nsudo c3os-agent upgrade \u0026lt;version\u0026gt; Use --force to force upgrading to avoid checking versions.\nIt is possible altough to use the same commandset from Elemental-toolkit. So for example, the following works too:\nsudo elemental upgrade --no-verify --docker-image quay.io/c3os/c3os:opensuse-v1.21.4-22 See also the general Elemental-toolkit documentation which applies for c3os as well.\n"
},
{
	"uri": "https://docs.c3os.io/installation/manual/",
	"title": "Manual installation",
	"tags": [],
	"description": "",
	"content": "To install manually, follow the quickstart. When the QR code is prompted at the screen, you should be able to login via SSH to the box with the password c3os as c3os user.\nNote, after the installation the password login is disabled, users and ssh keys to login must be configured via cloud-init.\n Installation To start the installation, run from the console:\nsudo elemental install --device /dev/sda --cloud-init $CONFIG Where the config can be a cloud-init file or a URL to it:\n#cloud-init c3os: network_token: .... # extra configuration "
},
{
	"uri": "https://docs.c3os.io/architecture/meta/",
	"title": "Meta-Distribution",
	"tags": [],
	"description": "",
	"content": "We like to define c3os as a meta Linux Distribution as its goal is to convert any other distro to an immutable layout with Kubernetes Native components.\nc3OS The c3OS stack is composed of the following:\n A core OS image release for each flavor in ISO, qcow2, and similar format (currently can pick from openSUSE and Alpine based) - provided for user convenience A release with k3s embedded A set of Kubernetes Native API components (CRDs) to install into the control-plane node, to manage deployment, artifacts creation, and lifecycle (WIP) A set of Kubernetes Native API components (CRDs) to install into the target nodes to manage and control the node after deployment (WIP) An agent installed into the nodes to be compliant with Kubernetes Native API components mentioned above  Every component is extensible and modular such as it can be customized and replaced in the stack, and built off either locally or with Kubernetes\nInternal components C3OS encompassess several components, some externally, most notably:\n k3s as a Kubernetes distribution edgevpn (optional) as fabric for the distributed network, node coordination and bootstrap. Provides also embedded DNS capabilities for the cluster. elemental-toolkit as a fundament to build the Linux derivative. Indeed, any Elemental docs applies to c3os as well. nohang A sophisticated low memory handler for Linux  "
},
{
	"uri": "https://docs.c3os.io/architecture/network/",
	"title": "P2P Network",
	"tags": [],
	"description": "",
	"content": "c3OS can automatically setup a VPN between the nodes using edgevpn. This also allows the nodes to automatically coordinate, discover/configure and establish a network overlay spanning across multiple regions.\nThe connection happens in 3 stages, where the discovery is driven by DHT and mDNS (which can be selectively disabled/enabled)\n Discovery Gossip network Full connectivity  "
},
{
	"uri": "https://docs.c3os.io/quickstart/",
	"title": "Quickstart",
	"tags": [],
	"description": "",
	"content": "Quickstart Provisioning of c3OS into baremetal, edge, embedded devices, and cloud.\n"
},
{
	"uri": "https://docs.c3os.io/upgrade/kubernetes/",
	"title": "Upgrading from Kubernetes",
	"tags": [],
	"description": "",
	"content": "c3OS upgrades can be driven either manually or via Kubernetes. In order to trigger upgrades it is required to apply a CRD to the target cluster for the upgrade.\nUpgrading from version X to version Y with Kubernetes To upgrade a node it is necessary system-upgrade-controller to be deployed in the target cluster.\nTo install it, use kubectl:\nkubectl apply -f https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml To trigger an upgrade, create a plan for the system-upgrade-controller which refers to the image version that we want to upgrade.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | kubectl apply -f - --- apiVersion: upgrade.cattle.io/v1 kind: Plan metadata: name: os-upgrade namespace: system-upgrade labels: k3s-upgrade: server spec: concurrency: 1 # This is the version (tag) of the image. # The version is refered to the c3os version plus the k3s version. version: \u0026#34;v0.57.0-k3sv1.23.9-k3s1\u0026#34; nodeSelector: matchExpressions: - {key: kubernetes.io/hostname, operator: Exists} serviceAccountName: system-upgrade cordon: false drain: force: false disableEviction: true upgrade: # Here goes the image which is tied to the flavor being used. # Currently can pick between opensuse and alpine image: quay.io/c3os/c3os-opensuse command: - \u0026#34;/usr/sbin/suc-upgrade\u0026#34; EOF To check all the available versions, see the images available on the container registry, corresponding to the flavor/version selected.\nSeveral upgrade strategies can be used with system-upgrade-controller which are not illustrated here in this example. For instance it can be specified the number of hosts which are running the upgrades, filtering by labels, and more. Refer to the project documentation on how to create efficient strategies to roll upgrades on the nodes. In the example above the upgrades are applied to every host of the cluster, one-by-one in sequence.\n A pod should appear right after which carries on the upgrade and automatically reboots the node:\n$ kubectl get pods -A ... system-upgrade apply-os-upgrade-on-c3os-with-1a1a24bcf897bd275730bdd8548-h7ffd 0/1 Creating 0 40s Done! we should have all the basic to get our first cluster rolling, but there is much more we can do.\nDon\u0026rsquo;t miss out how to create multi-machine clusters, or clusters using the p2p fully-meshed network.\n"
},
{
	"uri": "https://docs.c3os.io/installation/",
	"title": "Installation reference",
	"tags": [],
	"description": "",
	"content": "Installation Provisioning of c3os into nodes can be performed in different ways. This section guides you into provisioning c3os into your nodes by use-case.\n"
},
{
	"uri": "https://docs.c3os.io/installation/interactive/",
	"title": "Interactive installation",
	"tags": [],
	"description": "",
	"content": "The interactive installation can be accessed from the LiveCD and guides the user into the installation process.\nIt generates a configuration file, which is later accessible after installation at /oem/99_custom.yaml.\nFrom the boot menu When loading any c3os ISOs, a GRUB menu like the following will be displayed. To access the interactive installation, select the third entry (c3os (interactive install)).\nManually The interactive installer can be also started manually with c3os-agent interactive-install from the LiveCD.\n"
},
{
	"uri": "https://docs.c3os.io/reference/paths/",
	"title": "Paths",
	"tags": [],
	"description": "",
	"content": "The following paths are relevant for c3os:\n   Path Description     /usr/local/.c3os/deployed Sentinel file written after bootstrapping is complete. Remove to retrigger automatic bootstrap   /usr/local/.c3os/lease IP Lease of the node in the network. Delete to change IP address of the node    "
},
{
	"uri": "https://docs.c3os.io/installation/automated/",
	"title": "Automated installation",
	"tags": [],
	"description": "",
	"content": "It is possible to drive the installation automatically by configuring a specific portion of the configuration file (install). The configuration file can be supplied then in various way, by either creating an additional ISO to mount ( if a VM, or burn to USB stick if baremetal), specifying a config via URL or even create a ISO from a container image with an embedded config file, which we are going to explore here.\nThe install block can be used to customize the installation drive, reboot or shutdown, and additional bundles, for example:\ninstall: # Device for automated installs device: \u0026#34;/dev/sda\u0026#34; # Reboot after installation reboot: true # Power off after installation poweroff: true # Set to true to enable automated installations auto: true # A list of bundles bundles: - quay.io/c3os/packages/... Datasource The configuration file can be provided to c3os by mounting an ISO in the node with the cidata label. The ISO must contain a user-data (which contain your configuration) and meta-data file.\nConsider a cloud-init of the following content, which is configured to automatically install onto /dev/sda and reboot:\n#node-config install: device: \u0026#34;/dev/sda\u0026#34; reboot: true poweroff: false auto: true # Required, for automated installations c3os: network_token: .... # extra configuration Save it as cloud_init.yaml, and we will now create an ISO with it.\nTo create an ISO as datasource, run the following:\n$ mkdir -p build $ cd build $ touch meta-data $ cp -rfv cloud_init.yaml user-data $ mkisofs -output ci.iso -volid cidata -joliet -rock user-data meta-data Now the iso is ready to be attached as the CDROM to the machine, boot it up as usual along with the c3os iso.\nVia config URL It is possible to specify config_url=\u0026lt;URL\u0026gt; as boot argument during boot. This will let the machine pull down the configuration specified via URL and perform the installation with the configuration specified. The config will be available in the system after installation as usual at /oem/99_custom.yaml.\nIf you don\u0026rsquo;t know where to upload such config, it is common habit upload those as Github gists.\nISO remastering It is possible to create custom ISOs with an embedded cloud-config. This will let the machine automatically boot with a configuration file, which later will be installed in the system after provisioning is completed.\nLocally To remaster an ISO locally you just need docker.\nAs c3os is based on elemental, the elemental-cli can be used to create a new ISO, with an additional config, consider the following steps:\n$ IMAGE=\u0026lt;source/image\u0026gt; $ mkdir -p files-iso/boot/grub2 # You can replace this step with your own grub config. This GRUB configuration is the boot menu of the ISO $ wget https://raw.githubusercontent.com/c3os-io/c3os/master/overlay/files-iso/boot/grub2/grub.cfg -O files-iso/boot/grub2/grub.cfg # Copy the config file $ cp -rfv cloud_init.yaml files-iso/config.yaml # Pull the image locally $ docker pull $IMAGE # Optionally, modify the image here! # docker run --entrypoint /bin/bash --name changes -ti $IMAGE # docker commit changes $IMAGE # Build an ISO with $IMAGE $ docker run -v $PWD:/cOS -v /var/run/docker.sock:/var/run/docker.sock -i --rm quay.io/c3os/osbuilder-tools:v0.1.0 --name \u0026#34;custom-iso\u0026#34; --debug build-iso --date=false --local --overlay-iso /cOS/files-iso $IMAGE --output /cOS/ Kubernetes It is possible to create ISOs and derivatives using extended Kubernetes API resources with an embedded config file to drive automated installations.\nThis method also allows to tweak the container image by overlaying others on top - without breaking the concept of immutability and single image OS.\nConsider the following example, which requires a Kubernetes cluster to run the components, but works also on kind:\n# Adds the c3os repo to helm $ helm repo add c3os https://c3os-io.github.io/helm-charts \u0026#34;c3os\u0026#34; has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;c3os\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ # Install the CRD chart $ helm install c3os-crd c3os/c3os-crds NAME: c3os-crd LAST DEPLOYED: Tue Sep 6 20:35:34 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None # Installs osbuilder $ helm install c3os-osbuilder c3os/osbuilder NAME: c3os-osbuilder LAST DEPLOYED: Tue Sep 6 20:35:53 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None # Applies an OSArtifact spec cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; | kubectl apply -f - apiVersion: build.c3os-x.io/v1alpha1 kind: OSArtifact metadata: name: hello-c3os spec: imageName: \u0026#34;quay.io/c3os/c3os:opensuse-latest\u0026#34; iso: true bundles: - quay.io/c3os/packages:goreleaser-utils-1.11.1 grubConfig: | search --file --set=root /boot/kernel.xz set default=0 set timeout=10 set timeout_style=menu set linux=linux set initrd=initrd if [ \u0026#34;${grub_cpu}\u0026#34; = \u0026#34;x86_64\u0026#34; -o \u0026#34;${grub_cpu}\u0026#34; = \u0026#34;i386\u0026#34; -o \u0026#34;${grub_cpu}\u0026#34; = \u0026#34;arm64\u0026#34; ];then if [ \u0026#34;${grub_platform}\u0026#34; = \u0026#34;efi\u0026#34; ]; then if [ \u0026#34;${grub_cpu}\u0026#34; != \u0026#34;arm64\u0026#34; ]; then set linux=linuxefi set initrd=initrdefi fi fi fi if [ \u0026#34;${grub_platform}\u0026#34; = \u0026#34;efi\u0026#34; ]; then echo \u0026#34;Please press \u0026#39;t\u0026#39; to show the boot menu on this console\u0026#34; fi set font=($root)/boot/${grub_cpu}/loader/grub2/fonts/unicode.pf2 if [ -f ${font} ];then loadfont ${font} fi menuentry \u0026#34;install\u0026#34; --class os --unrestricted { echo Loading kernel... $linux ($root)/boot/kernel.xz cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable vga=795 nomodeset nodepair.enable echo Loading initrd... $initrd ($root)/boot/rootfs.xz } if [ \u0026#34;${grub_platform}\u0026#34; = \u0026#34;efi\u0026#34; ]; then hiddenentry \u0026#34;Text mode\u0026#34; --hotkey \u0026#34;t\u0026#34; { set textmode=true terminal_output console } fi cloudConfig: | #node-config install: device: \u0026#34;/dev/sda\u0026#34; reboot: true poweroff: false auto: true # Required, for automated installations EOF # Note on running with kind: $ IP=$(docker inspect kind-control-plane | jq -r \u0026#39;.[0].NetworkSettings.Networks.kind.IPAddress\u0026#39;) $ PORT=$(kubectl get svc hello-c3os -o json | jq \u0026#39;.spec.ports[0].nodePort\u0026#39;) $ curl http://$IP:$PORT/hello-c3os.iso -o test.iso "
},
{
	"uri": "https://docs.c3os.io/reference/cli/",
	"title": "CLI",
	"tags": [],
	"description": "",
	"content": "A CLI is provided as part of releases associated to each c3os version.\nThe CLI can be used from an external machine to generate network tokens and pair nodes on first-boot.\n./c3os --help NAME: c3os - c3os (register|install) USAGE: [global options] command [command options] [arguments...] VERSION: 0.1 DESCRIPTION: c3os registers and installs c3os boxes AUTHOR: Ettore Di Giacinto COMMANDS: register create-config, c generate-token, g setup, s get-kubeconfig install, i help, h Shows a list of commands or help for one command create-config Generates a new c3os configuration file which can be used as cloud-init, with a new unique network token:\n$ ./c3os create-config c3os: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IEVCMzJJMlNXTjJCNFBHNEtCWTNBUVBBS0FWRTY0Q0VLVUlDTktTUFVWVU5BWTM0QklEQ0EKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBDMk1RRk5DWEFVRElPWjVHM1pZUUIzVEVHTzVXVEdQR1pZSEVQQkY3SFEyVUROUlZCTkxRCiAgICBsZW5ndGg6IDMyCnJvb206IGp6Q29kQVVOWUZSUklQU3JISmx4d1BVUnVxTGJQQnh4CnJlbmRlenZvdXM6IG5NckRCbllyVVBMdnFPV0Z2dWZvTktXek1adEJIRmpzCm1kbnM6IGpQUUhIbVZza2x6V29xbWNkeVlnbVhMSVFjTE1HUFN6Cm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \u0026quot;\u0026quot; poweroff: false Now you can use this in your configuration file to create new c3os nodes:\nc3os: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IEVCMzJJMlNXTjJCNFBHNEtCWTNBUVBBS0FWRTY0Q0VLVUlDTktTUFVWVU5BWTM0QklEQ0EKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBDMk1RRk5DWEFVRElPWjVHM1pZUUIzVEVHTzVXVEdQR1pZSEVQQkY3SFEyVUROUlZCTkxRCiAgICBsZW5ndGg6IDMyCnJvb206IGp6Q29kQVVOWUZSUklQU3JISmx4d1BVUnVxTGJQQnh4CnJlbmRlenZvdXM6IG5NckRCbllyVVBMdnFPV0Z2dWZvTktXek1adEJIRmpzCm1kbnM6IGpQUUhIbVZza2x6V29xbWNkeVlnbVhMSVFjTE1HUFN6Cm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \u0026#34;\u0026#34; poweroff: false # Cloud init syntax to setup users.  # See https://rancher.github.io/elemental-toolkit/docs/reference/cloud_init/ stages: network: - name: \u0026#34;Setup users\u0026#34; authorized_keys: c3os: - github:yourhandle! generate-token Generates a new network token which can be used in a configuration file:\n$ ./c3os generate-token b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IFhMMjRYUk1MTlFOQ1pJQTU0SVFLQ1laMk83SENQWEFBU1ZKN0tZSTQ3MzVaUkpKSktRSEEKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBMR1dMWFBTUllaU0ZERDdOT0pBNzdKV0ZWQjRHVkZBMjJIWlZPWU1VT0lNSFVYNFZXUURRCiAgICBsZW5ndGg6IDMyCnJvb206IFRtcUt5VnFHQ1ZZam9TRm9CTEVNRGVEdmJzelBkVEdoCnJlbmRlenZvdXM6IGttb3J4Q21sY2NjVVppWmdkSW5xTERvTGJtS3ZGdm9mCm1kbnM6IEZkWVdQc2R4aHdvWHZlb0VzSXNnVHRXbEJUbE9IVHJmCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== And now:\nc3os: network_token: b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MjIzMzcyMDM2ODU0Nzc1ODA3CiAgICBrZXk6IFhMMjRYUk1MTlFOQ1pJQTU0SVFLQ1laMk83SENQWEFBU1ZKN0tZSTQ3MzVaUkpKSktRSEEKICAgIGxlbmd0aDogMzIKICBjcnlwdG86CiAgICBpbnRlcnZhbDogOTIyMzM3MjAzNjg1NDc3NTgwNwogICAga2V5OiBMR1dMWFBTUllaU0ZERDdOT0pBNzdKV0ZWQjRHVkZBMjJIWlZPWU1VT0lNSFVYNFZXUURRCiAgICBsZW5ndGg6IDMyCnJvb206IFRtcUt5VnFHQ1ZZam9TRm9CTEVNRGVEdmJzelBkVEdoCnJlbmRlenZvdXM6IGttb3J4Q21sY2NjVVppWmdkSW5xTERvTGJtS3ZGdm9mCm1kbnM6IEZkWVdQc2R4aHdvWHZlb0VzSXNnVHRXbEJUbE9IVHJmCm1heF9tZXNzYWdlX3NpemU6IDIwOTcxNTIwCg== offline: false reboot: false device: \u0026#34;\u0026#34; poweroff: false # Cloud init syntax to setup users.  # See https://rancher.github.io/elemental-toolkit/docs/reference/cloud_init/ stages: network: - name: \u0026#34;Setup users\u0026#34; authorized_keys: c3os: - github:yourhandle! register The register command can be used to register and drive installation of nodes via QR code with a cloud-init config file ( with --config).\nNAME: register - USAGE: register [command options] [arguments...] OPTIONS: --config value --device value --reboot --poweroff When booting c3os via ISO, the boot process ends up in displaying a QR code which can be parsed by c3os register from another machine.\nTaking a screenshot register by default takes a screenshot and tries to find a QR code in it:\nc3os register Providing a qrcode image/screenshot manually It can be also be specified an image:\nc3os register \u0026lt;file.png\u0026gt; After the pairing is done, the node will start installation with the provided options.\nA --device and a --config file are required in order to have a functional installation.\nbridge Connect to the nodes in the VPN p2p network by creating a tun device on the host.\nIt needs a --network-token($NETWORK_TOKEN) argument and exposes an API endpoint available at localhost:8080 to monitor the network status.\ninstall Called by c3os nodes on boot and not meant to be used manually. It kicks in the installation and the QR pairing process.\nsetup Called by c3os nodes on boot and not meant to be used manually. It prepares edgevpn and k3s bootstrapping the node and the VPN.\n"
},
{
	"uri": "https://docs.c3os.io/reference/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Root permission By default there is no root user set. A default user (c3os) is created and can use sudo without password authentication during LiveCD bootup.\nGet kubeconfig On all nodes of the cluster it\u0026rsquo;s possible to invoke c3os get-kubeconfig to recover the kubeconfig file\nConnect to the cluster network Network tokens can be used to connect to the VPN created by the cluster. They are indeed tokens of edgevpn networks, and thus can be used to connect to with its CLI.\nThe c3os CLI can be used to connect as well, with the bridge command:\nsudo c3os bridge --network-token \u0026lt;TOKEN\u0026gt;  The command needs root permissions as it sets up a local tun interface to connect to the VPN.\n Afterward you can connect to localhost:8080 to access the network API and verify machines are connected.\nSee edgeVPN documentation on how to connect to the VPN with the edgeVPN cli, which is similar:\nEDGEVPNTOKEN=\u0026lt;network_token\u0026gt; edgevpn --dhcp Setup process c3os node at first boot will start the c3os-agent service, you can always check what\u0026rsquo;s happening by running journalctl -fu c3os-agent.\nThis service will setup k3s and edgevpn dynamically on first-boot, once it configures the machine it does not run on boot anymore, unless /usr/local/.c3os/deployed is removed..\nThose are the steps executed in sequence by the c3os-agent service:\n Will create a edgevpn@c3os service and enabled on start. The configuration for the connection is stored in /etc/systemd/system.conf.d/edgevpn-c3os.env and depends on the cloud-init configuration file provided during installation time Automatic role negotiation starts, nodes will co-ordinate for an IP and a role Once roles are defined a node will either set the k3s or k3s-agent service. Configuration for each service is stored in /etc/sysconfig/k3s and /etc/sysconfig/k3s-agent respectively  "
},
{
	"uri": "https://docs.c3os.io/upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "",
	"content": "Installing c3os on baremetal, VMs, \u0026hellip; Automated, Paired and take over install "
},
{
	"uri": "https://docs.c3os.io/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Installing c3os on baremetal, VMs, \u0026hellip; Automated, Paired and take over install "
},
{
	"uri": "https://docs.c3os.io/reference/configuration/",
	"title": "Configuration reference",
	"tags": [],
	"description": "",
	"content": "Here you can find a full reference of the fields available to configure a c3os node\n#node-config # The c3os block enables the p2p full-mesh functionalities. # To disable, don\u0026#39;t specify one. c3os: # This is a network token used to establish the p2p full meshed network. # Don\u0026#39;t specify one to disable full-mesh functionalities. network_token: \u0026#34;....\u0026#34; # Manually set node role. Available: master, worker. Defaults auto (none). This is available  role: \u0026#34;master\u0026#34; # User defined network-id. Can be used to have multiple clusters in the same network network_id: \u0026#34;dev\u0026#34; # Enable embedded DNS See also: https://mudler.github.io/edgevpn/docs/concepts/overview/dns/ dns: true # The install block is to drive automatic installations without user interaction. install: # Device for automated installs device: \u0026#34;/dev/sda\u0026#34; # Reboot after installation reboot: true # Power off after installation poweroff: true # Set to true when installing without Pairing auto: true # Add bundles in runtime bundles: - ... # Set grub options grub_options: key: value vpn: # EdgeVPN environment options DHCP: \u0026#34;true\u0026#34; # Disable DHT (for airgap) EDGEVPNDHT: \u0026#34;false\u0026#34; EDGEVPNMAXCONNS: \u0026#34;200\u0026#34; # If DHCP is false, it\u0026#39;s required to be given a specific node IP. Can be arbitrary ADDRESS: \u0026#34;10.2.0.30/24\u0026#34; # See all EDGEVPN options: # - https://github.com/mudler/edgevpn/blob/master/cmd/util.go#L33 # - https://github.com/mudler/edgevpn/blob/master/cmd/main.go#L48 k3s: # Additional env/args for k3s server instances env: K3S_RESOLV_CONF: \u0026#34;\u0026#34; K3S_DATASTORE_ENDPOINT: \u0026#34;mysql://username:password@tcp(hostname:3306)/database-name\u0026#34; args: - --label \u0026#34;\u0026#34; - --data-dir \u0026#34;\u0026#34; # Enabling below it replaces args/env entirely # replace_env: true # replace_args: true k3s-agent: # Additional env/args for k3s agent instances env: K3S_NODE_NAME: \u0026#34;foo\u0026#34; args: - --private-registry \u0026#34;...\u0026#34; # Enabling below it replaces args/env entirely # replace_env: true # replace_args: true # Additional cloud init syntax can be used here. # See https://rancher.github.io/elemental-toolkit/docs/reference/cloud_init/ for a complete reference stages: network: - name: \u0026#34;Setup users\u0026#34; authorized_keys: c3os: - github:mudler Syntax c3os supports the standard cloud-init syntax and the extended one from the Elemental-toolkit which is based on yip.\nExamples using the extended notation for running k3s as agent or server are in examples.\nk3s The k3s and the k3s-agent block are used to customize the environment and arg settings of k3s, consider:\nk3s: enabled: true # Additional env/args for k3s server instances env: K3S_RESOLV_CONF: \u0026#34;\u0026#34; K3S_DATASTORE_ENDPOINT: \u0026#34;mysql://username:password@tcp(hostname:3306)/database-name\u0026#34; args: - --cluster-init for agent:\nk3s-agent: enabled: true # Additional env/args for k3s server instances env: K3S_RESOLV_CONF: \u0026#34;\u0026#34; K3S_DATASTORE_ENDPOINT: \u0026#34;mysql://username:password@tcp(hostname:3306)/database-name\u0026#34; args: - --cluster-init See also the examples folder in the repository to configure k3s manually.\ninstall.grub_options Is a map of key/value grub options to be set in the grub environment after installation.\nIt can be used to set additional boot arguments on boot, consider to set panic=0 as bootarg:\n#node-config install: # See also: https://rancher.github.io/elemental-toolkit/docs/customizing/configure_grub/#grub-environment-variables grub_options: extra_cmdline: \u0026#34;panic=0\u0026#34; Below a full list of all the available options:\n   Variable Description     next_entry Set the next reboot entry   saved_entry Set the default boot entry   default_menu_entry Set the name entries on the GRUB menu   extra_active_cmdline Set additional boot commands when booting into active   extra_passive_cmdline Set additional boot commands when booting into passive   extra_recovery_cmdline Set additional boot commands when booting into recovery   extra_cmdline Set additional boot commands for all entries   default_fallback Sets default fallback logic    c3os.dns When c3os.dns is set to true embedded DNS is configured on the node. This allows to propagate custom records to the nodes by using the blockchain DNS server, for example, assuming c3os bridge is running in a separate terminal:\ncurl -X POST http://localhost:8080/api/dns --header \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;Regex\u0026#34;: \u0026#34;foo.bar\u0026#34;, \u0026#34;Records\u0026#34;: { \u0026#34;A\u0026#34;: \u0026#34;2.2.2.2\u0026#34; } }\u0026#39; Will add the foo.bar domain with 2.2.2.2 as A response.\nEvery node with dns enabled will be able to resolve the domain after the domain is correctly announced.\nYou can check out the dns in the DNS page in the API, see also the EdgeVPN docs.\nFurthermore, is possible to tweak DNS server which are used to forward requests for domain listed outside, and as well it\u0026rsquo;s possible to lock down resolving only to nodes in the blockchain, by customizing the configuration file:\n#cloud-config c3os: network_token: \u0026#34;....\u0026#34; # Enable embedded DNS See also: https://mudler.github.io/edgevpn/docs/concepts/overview/dns/ dns: true vpn: # Disable DNS forwarding DNSFORWARD: \u0026#34;false\u0026#34; # Set cache size DNSCACHESIZE: \u0026#34;200\u0026#34; # Set DNS forward server DNSFORWARDSERVER: \u0026#34;8.8.8.8:53\u0026#34; "
},
{
	"uri": "https://docs.c3os.io/installation/p2p/",
	"title": "Full P2P mesh support",
	"tags": [],
	"description": "",
	"content": " This feature is crazy and experimental!\n This section will guide on how to leverage the p2p full-mesh capabilities of c3os.\nc3OS supports p2p full-mesh out of the box. That allows to seamelessly interconnect clusters and nodes from different regions into an unified overlay network, additionally, the same network is used for co-ordinating nodes automatically, allowing self-automated node bootstrap.\nA hybrid network is automatically set up between all the nodes, as such there is no need to expose them over the Internet, and either expose the Kubernetes management API outside, reducing attacker\u0026rsquo;s exploiting surface.\nC3os can be configured to automatically bootstrap a kubernetes cluster with the full-mesh functionalities, or just either add an additional interface to the machines to let them communicate within a new network segment.\nIf you are not familiar with the process, it is suggested to follow the quickstart first, and the steps below in sequence. The section below just explains the difference in the configuration options to enable p2p full-mesh during the installation phase.\nPrerequisites  c3os-cli  Configuration In order to configure a node to join over the same p2p network during installation add a c3os block in the configuration, like the following:\nc3os: network_token: \u0026#34;....\u0026#34; # Optionally, set a network id (for multiple clusters in the same network) # network_id: \u0026#34;dev\u0026#34; # Optionally set a role # role: \u0026#34;master\u0026#34; The c3os block is used to configure settings to the mesh functionalities. The minimum required argument is the network_token.\nnetwork_token The network_token is a unique, shared secret which is spread over the nodes and can be generated with the c3os-cli. It will make all the node connect automatically to the same network. Every node will generate a set of private/public key keypair automatically on boot that are used to communicate securely within a e2e encrypted channel.\nTo generate a new network token, you can use the c3os-cli:\nc3os generate-token network_id An optional, unique identifier for the cluster. This allows to bootstrap multiple cluster over the same underlaying network.\nrole Force a role for the node. Available: worker, master.\nFor a full reference of all the supported use cases, see cloud-init.\nJoin new nodes To join new nodes, simply re-apply the process to new nodes by specifying the same config.yaml for all the machines. Unless you have specified a role for each of the nodes, the configuration doesn\u0026rsquo;t need any further change. The machines will connect automatically between themselves, either remotely on local network.\nConnect to the nodes The c3os-cli can be used to establish a tunnel with the nodes network given a network_token.\nsudo c3os bridge --network-token \u0026lt;TOKEN\u0026gt; This command will create a tun device in your machine and will make possible to contact each node in the cluster.\nThe command requires root permissions in order to create a tun/tap device on the host\n An API will be also available at localhost:8080 for inspecting the network status.\nGet kubeconfig To get the kubeconfig it is sufficient or either login to the master node and get it from the engine ( e.g. k3s puts it /etc/rancher/k3s/k3s.yaml) or using the c3os cli.\nBy using the CLI, you need to be connected to the bridge, or either logged in from one of the nodes and perform the commands in the console.\nIf you are using the CLI, you need to run the bridge in a separate window.\nTo get the kubeconfig, run the following:\nc3os get-kubeconfig \u0026gt; kubeconfig  c3os bridge acts like kubectl proxy. you need to keep it open to operate the kubernetes cluster and access the API.\n "
},
{
	"uri": "https://docs.c3os.io/reference/recovery_mode/",
	"title": "Remote Recovery mode",
	"tags": [],
	"description": "",
	"content": "The c3os recovery mode can be used to recover a damaged system, or to regain access remotely (with assistance) to a machine which has been lost access to. The recovery mode is accessible only from the GRUB menu, from both the LiveCD and an installed system.\nOn installed system there are two recovery modes available during boot. Below it is described only how the c3os remote recovery works. The manual recovery entry has nothing special from the standard Elemental-toolkit recovery mode. It can be used to reset the A/B partitions (with the user/pass used during setup) and perform any other operation without remote access.\n Boot into recovery mode c3os recovery mode can be accessed either via ISO or from an installed system.\nA GRUB menu will be displayed: Select the last entry c3os (remote recovery mode) and press enter.\nAt this point the boot process starts and you should be welcomed by the c3os screen:\nAfter few second the recovery process starts, and right after a QR code will be printed out of the screen along with a password which can be used later on to SSH into the machine:\nAt this stage, take a screenshot or a photo, just save the image with the QR code.\nConnect to the machine In another machine you are using to connect to your server (your workstation, a jumpbox, or ..) use the c3os CLI to connect over the remote machine:\n$ ./c3os bridge --qr-code-image /path/to/image.png INFO Connecting to service kAIsuqiwKR INFO SSH access password is yTXlkak INFO SSH server reachable at 127.0.0.1:2200 INFO To connect, keep this terminal open and run in another terminal 'ssh 127.0.0.1 -p 2200' the password is yTXlkak INFO Note: the connection might not be available instantly and first attempts will likely fail. INFO Few attempts might be required before establishing a tunnel to the host. INFO Starting EdgeVPN network INFO Node ID: 12D3KooWSTRBCTNGZ61wzK5tgYvFi8rQVxkXJCDUYngBWGDSyoBK INFO Node Addresses: [/ip4/192.168.1.233/tcp/36071 /ip4/127.0.0.1/tcp/36071 /ip6/::1/tcp/37661] INFO Bootstrapping DHT At this point the bridge should start, and you should be able to see connection messages in the terminal. You can connect to the remote machine by using ssh and pointing it locally at 127.0.0.1:2200. The username is not relevant, the password is print from the CLI.\nThe bridge operates in the foreground, so you have to kill by hitting CTRL-C.\n"
},
{
	"uri": "https://docs.c3os.io/reference/customizing/",
	"title": "Customizing the system image",
	"tags": [],
	"description": "",
	"content": "c3os is a container-based OS, if you want to change c3os and add a package it is required to build only a docker image.\nFor example:\nFROMquay.io/c3os/c3os:opensuse-latestRUN zypper in -y ...RUN export VERSION=\u0026#34;my-version\u0026#34;RUN envsubst \u0026#39;${VERSION}\u0026#39; \u0026lt;/etc/os-releaseThe image can be then used with c3os upgrade or with system-upgrade-controller for upgrades within Kubernetes.\n"
},
{
	"uri": "https://docs.c3os.io/examples/",
	"title": "Examples",
	"tags": [],
	"description": "",
	"content": "This page lists a set of tasks that can be performed after you install c3OS.\nEach task can be done separately; however, if you\u0026rsquo;re not familiar with c3OS, or Kubernetes in general, we recommend that you follow the tasks below in sequential order.\n"
},
{
	"uri": "https://docs.c3os.io/installation/raspberry/",
	"title": "Raspberry",
	"tags": [],
	"description": "",
	"content": "c3os supports Rasperry Pi model 3 and 4 with 64bit architecture.\nYou can find arm64 raspberry images in the releases page. For example https://github.com/c3os-io/c3os/releases/download/v1.21.4-35/c3os-opensuse-arm-rpi-v1.21.4-35.img.tar.xz.\nFlash the image into a SD card with dd or Etcher and place your cloud-init configuration file inside the cloud-config directory ( create it if not present ) into the COS_PERSISTENT partition, for example cloud-config/cloud-init.yaml.\n"
},
{
	"uri": "https://docs.c3os.io/examples/examples/",
	"title": "Common setup",
	"tags": [],
	"description": "",
	"content": "In the following section you can find example configuration files to achieve specific c3os setups.\nSingle node cluster By default c3os requires multiple nodes. As for the c3os decentralized nature, it requires co-ordination between at least 2 nodes to achieve consensus on IPs, network setting, etc.\nIn order to create single-node cluster, we need to force both the role and the ip by disabling DHCP:\nc3os: network_token: \u0026#34;....\u0026#34; role: \u0026#34;master\u0026#34; vpn: # EdgeVPN environment options DHCP: \u0026#34;false\u0026#34; ADDRESS: \u0026#34;10.1.0.2/24\u0026#34; Note, the same setup can be used to specify master nodes in a set, as to join nodes it is still possible without specifying any extra setting:\nc3os: network_token: \u0026#34;....\u0026#34; As always, IPs here are arbitrary as they are virtual ips in the VPN which is created between the cluster nodes.\nRun only k3s without VPNs c3os can be also used without any VPN and P2P network. Infact, k3s is already pre-installed, and it is sufficient to not specify any c3os block in the cloud init configuration.\nFor example, to start k3s as a server with c3os it\u0026rsquo;s sufficient to specify the k3s service in the config file:\n#node-config k3s: enabled: true And similarly for an agent:\n#node-config k3s-agent: enabled: true env: K3S_TOKEN: ... K3S_URL: ... Single node cluster with default user/password This is will setup k3s single-node + VPN with a static ip (10.1.0.2).\nc3os: network_token: \u0026#34;....\u0026#34; role: \u0026#34;master\u0026#34; vpn: # EdgeVPN environment options DHCP: \u0026#34;false\u0026#34; ADDRESS: \u0026#34;10.1.0.2/24\u0026#34; stages: initramfs: - name: \u0026#34;Set user and password\u0026#34; users: c3os: passwd: \u0026#34;c3os\u0026#34; Hostname Sometimes you might want to create a single cloud-init file for a set of machines, and also make sure each node has a different hostname.\nThe cloud-config syntax supports templating, so one could automate hostname generation based on the machine id which is generated for each host:\n#node-config stages: initramfs: - name: \u0026#34;Setup hostname\u0026#34; hostname: \u0026#34;node-{{ trunc 4 .MachineID }}\u0026#34; "
},
{
	"uri": "https://docs.c3os.io/examples/configuration/",
	"title": "Configuring a node after install",
	"tags": [],
	"description": "",
	"content": "c3os configuration mechanism is all based on cloud-config file.\nBy default, c3os reads in lexicographic order YAML cloud-config files in the /usr/local/cloud-config and /oem directories.\nFor instance, you should be able to see the configuration generated by the interactive-installer in the /oem/99_custom.yaml config file.\nThis mechanism can be used to set and enable persistent configuration on boot.\nExamples Below you will find some examples on how to use the cloud-config mechanism to enable/disable specific features.\nEnabling ZRAM on boot Copy the following file in /oem/100_zram.yaml or /usr/local/cloud-config/100_zram.yaml:\nstages: boot: - name: \u0026#34;zram setup\u0026#34; commands: - modprobe zram - echo lzo \u0026gt; /sys/block/zram0/comp_algorithm - echo 1G \u0026gt; /sys/block/zram0/disksize - mkswap --label zram0 /dev/zram0 - swapon --priority 100 /dev/zram0 name: \u0026#34;zfs setup\u0026#34; This YAML will run the commands on boot enabling zram as swap.\n"
},
{
	"uri": "https://docs.c3os.io/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "Basics "
},
{
	"uri": "https://docs.c3os.io/installation/takeover/",
	"title": "Take over installation",
	"tags": [],
	"description": "",
	"content": "c3os supports takeover installations, see also the Elemental-toolkit docs here are few summarized steps:\n From the Dedicated control panel (OVH, Hetzner, etc.), boot in rescue mode install docker and run for example:  export DEVICE=/dev/sda export IMAGE=quay.io/mudler/c3os:v1.21.4-19 # A url pointing to a valid cloud-init config file. E.g. as a gist at gists.github.com export CONFIG_FILE=... docker run --privileged -v $DEVICE:$DEVICE -ti $IMAGE cos-installer --config $CONFIG_FILE --no-cosign --no-verify --docker-image $IMAGE $DEVICE  Switch back to booting from HD and reboot  "
},
{
	"uri": "https://docs.c3os.io/installation/netboot/",
	"title": "Netboot installation",
	"tags": [],
	"description": "",
	"content": " This section is a work in progress!\n "
},
{
	"uri": "https://docs.c3os.io/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Welcome Welcome to the c3os documentation!\nc3OS is an open-source project which brings Edge, cloud and bare metal lifecycle OS management into the same design principles with a unified Kubernetes native API.\nIn a glance:\n Community Driven Open Source Meta-Distribution, Distro agnostic Immutable Secure Container based P2P Mesh  c3OS can be used to:\n Easily spin up a Kubernetes cluster, with the Linux distribution of your choice Manage the cluster lifecycle with Kubernetes - from building, to provisioning and upgrading Create a multiple-node single cluster which spans up across regions  To get familiar with c3os, check out the quickstart.\nWhat is it ? c3OS is a Kubernetes native, meta-Linux distribution that can be built, managed, and run with Kubernetes.\nWhy/when should I use it?\n Build your Cloud on-prem, no vendor-lock in, completely Open Source Brings same convenience of public cloud on premises Node provisioning, by bringing your own image or just use the c3os releases. For appliances that doesn\u0026rsquo;t have to be Kubernetes application specific - its design fits multiple use case scenarios  Features  At the current state c3OS can create multiple-node Kubernetes cluster with k3s - all k3s features are supported Upgrades can be done manually via CLI or with Kubernetes. Distribution of upgrades are done via container registries. An Immutable distribution which you can configure to your needs, while keep staying immutable Node configuration via a single cloud-init config file. Handle airgap upgrades with in-cluster container registries Extend the image in runtime or build time via Kubernetes Native API Plans to support CAPI, with full device lifecycle management Plans to support up to rke2, kubeadm, and much more! Nodes can optionally connect autonomously via full-mesh p2p hybrid VPN network. It allows to stretch a cluster up to 10000 km! c3OS can create private virtual network segments to enhance your cluster perimeter without any SPOF.  More than a Linux distribution c3OS is available as ISO, qcow2 and netboot artifact for user convenience based from Alpine and openSUSE, but it is actually more than that. It allows to turn any Linux distribution into a uniform, comformant distro with immutable design. As such, any distro which is \u0026ldquo;converted\u0026rdquo; will share the same, common feature set between all of them, and they are managed in the same way by Kubernetes Native API components.\nAny input OS will inherit:\n Immutability A/B upgrades Booting mechanism Fallback Boot assessment Single image, container based atomic upgrades Cloud init support All the c3OS feature-set  C3os treats all the OSes homogeneously in a distro-agnostic fashion.\nThe OS is a container image. That means that upgrades to nodes are distributed via container registries.\nInstallations medium and other assets required to boot baremetal or Edge devices are built dynamically by the Kubernetes Native API components provided by c3os.\nGoals The c3OS ultimate goal is to bridge the gap between Cloud and Edge by creating a smooth user experience. There are several areas in the ecosystem that can be improved for edge deployments to make it in pair with the cloud.\nThe c3OS project encompassess all the tools and architetural pieces needed to fill those gaps. This spans between providing Kubernetes Native API components to assemble OSes, deliver upgrades, and control nodes after deployment.\nc3OS is distro-agnostic, and embraces openness: the user can provide their own underlaying base image, and c3os onboards it and takes it over to make it Cloud Native, Immutable that plugs into an already rich ecosystem by leveraging containers as distribution medium.\nContribute c3OS is an open source project, and any contribution is more than welcome! The project is big and narrows to various degree of complexity and problem space. Feel free to join our chat, discuss in our forums and join us in the Office hours\nWe have an open roadmap, so you can always have a look on what\u0026rsquo;s going on, and actively contribute to it.\nUseful links:\n Upcoming releases  Community You can find us at:\n #c3os at matrix.org IRC #c3os in libera.chat Github Discussions  Project Office Hours Project Office Hours is an opportunity for attendees to meet the maintainers of the project, learn more about the project, ask questions, learn about new features and upcoming updates.\nOffice hours are happening weekly on Wednesday - 5:30 – 6:00pm CEST. Meeting link\nBesides we have monthly meetup to partecipate actively into the roadmap planning and presentation:\nRoadmap planning We will discuss on agenda items and groom issues, where we plan where they fall into the release timeline.\nOccurring: Monthly on the first Wednesday - 5:30 – 6:30pm CEST. Meeting link\nRoadmap presentation We will discuss the items of the roadmaps and the expected features on the next releases\nOccurring: Monthly on the second Wednesday - 5:30pm CEST Meeting link\nAlternatives There are other projects that are similar to c3os which are great and worth to mention, and actually c3os took to some degree inspiration from. However, c3os have different goals and takes completely unique approaches to the underlying system, upgrade and node lifecycle management.\n k3os Talos FlatCar CoreOS  Development Building c3os Requirements: Needs only docker.\nRun ./earthly.sh +all --FLAVOR=opensuse, should produce a docker image along with a working ISO\nWhat\u0026rsquo;s next? See the quickstart to install c3os on a VM and create a Kubernetes cluster!\n"
},
{
	"uri": "https://docs.c3os.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.c3os.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]